{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#####准备input#####\n",
    "import collections\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import find_contours\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import keras.backend as K\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.models import load_model\n",
    "#from keras.models import load_weights\n",
    "from collections.abc import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import (Conv3D, BatchNormalization, AveragePooling3D, concatenate, Lambda, SpatialDropout3D,\n",
    "                          Activation, Input, GlobalAvgPool3D, Dense, Conv3DTranspose, add)\n",
    "from keras.regularizers import l2 as l2_penalty\n",
    "from tensorflow.keras.models import Model\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "######准备各种工具函数#######\n",
    "####type1 gpu 调用函数###\n",
    "def get_gpu_session(ratio=None, interactive=False):\n",
    "    config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
    "    if ratio is None:\n",
    "        config.gpu_options.allow_growth = True\n",
    "    else:\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = ratio\n",
    "    if interactive:\n",
    "        sess = tf.InteractiveSession(config=config)\n",
    "    else:\n",
    "        sess = tf.compat.v1.Session(config=config)\n",
    "    return sess\n",
    "\n",
    "def set_gpu_usage(ratio=None):\n",
    "    sess = get_gpu_session(ratio)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######type2  计算各种精度指标的函数  ######\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    \"\"\"Computes the F score.\n",
    "    The F score is the weighted harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    F1 score: https://en.wikipedia.org/wiki/F1_score\n",
    "    \"\"\"\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    \"\"\"\n",
    "    return fbeta_score(y_true, y_pred, beta=1)\n",
    "\n",
    "\n",
    "def invasion_acc(y_true, y_pred):\n",
    "    binary_truth = y_true[:, -2] + y_true[:, -1]\n",
    "    binary_pred = y_pred[:, -2] + y_pred[:, -1]\n",
    "    return binary_accuracy(binary_truth, binary_pred)\n",
    "\n",
    "\n",
    "def invasion_precision(y_true, y_pred):\n",
    "    binary_truth = y_true[:, -2] + y_true[:, -1]\n",
    "    binary_pred = y_pred[:, -2] + y_pred[:, -1]\n",
    "    return precision(binary_truth, binary_pred)\n",
    "\n",
    "\n",
    "def invasion_recall(y_true, y_pred):\n",
    "    binary_truth = y_true[:, -2] + y_true[:, -1]\n",
    "    binary_pred = y_pred[:, -2] + y_pred[:, -1]\n",
    "    return recall(binary_truth, binary_pred)\n",
    "\n",
    "\n",
    "def invasion_fmeasure(y_true, y_pred):\n",
    "    binary_truth = y_true[:, -2] + y_true[:, -1]\n",
    "    binary_pred = y_pred[:, -2] + y_pred[:, -1]\n",
    "    return fmeasure(binary_truth, binary_pred)\n",
    "\n",
    "\n",
    "def ia_acc(y_true, y_pred):\n",
    "    binary_truth = y_true[:, -1]\n",
    "    binary_pred = y_pred[:, -1]\n",
    "    return binary_accuracy(binary_truth, binary_pred)\n",
    "\n",
    "\n",
    "def ia_precision(y_true, y_pred):\n",
    "    binary_truth = y_true[:, -1]\n",
    "    binary_pred = y_pred[:, -1]\n",
    "    return precision(binary_truth, binary_pred)\n",
    "\n",
    "\n",
    "def ia_recall(y_true, y_pred):\n",
    "    binary_truth = y_true[:, -1]\n",
    "    binary_pred = y_pred[:, -1]\n",
    "    return recall(binary_truth, binary_pred)\n",
    "\n",
    "\n",
    "def ia_fmeasure(y_true, y_pred):\n",
    "    binary_truth = y_true[:, -1]\n",
    "    binary_pred = y_pred[:, -1]\n",
    "    return fmeasure(binary_truth, binary_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######type3  相关损失函数######\n",
    "class DiceLoss:\n",
    "    def __init__(self, beta=1., smooth=1.):\n",
    "        self.__name__ = 'dice_loss_' + str(int(beta * 100))\n",
    "        self.beta = beta  # the more beta, the more recall\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        bb = self.beta * self.beta\n",
    "        y_true_f = K.batch_flatten(y_true)\n",
    "        y_pred_f = K.batch_flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f, axis=-1)\n",
    "        weighted_union = bb * K.sum(y_true_f, axis=-1) + \\\n",
    "                         K.sum(y_pred_f, axis=-1)\n",
    "        score = -((1 + bb) * intersection + self.smooth) / \\\n",
    "                (weighted_union + self.smooth)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######type4 各种工具函数####\n",
    "def find_edges(mask, level=0.5):\n",
    "    edges = find_contours(mask, level)[0]\n",
    "    ys = edges[:, 0]\n",
    "    xs = edges[:, 1]\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def plot_contours(arr, aux, level=0.5, ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1, **kwargs)\n",
    "    ax.imshow(arr, cmap=plt.cm.gray)\n",
    "    xs, ys = find_edges(aux, level)\n",
    "    ax.plot(xs, ys)\n",
    "\n",
    "\n",
    "def crop_at_zyx_with_dhw(voxel, zyx, dhw, fill_with):\n",
    "    '''Crop and pad on the fly.'''\n",
    "    shape = voxel.shape\n",
    "    # z, y, x = zyx\n",
    "    # d, h, w = dhw\n",
    "    crop_pos = []\n",
    "    padding = [[0, 0], [0, 0], [0, 0]]\n",
    "    for i, (center, length) in enumerate(zip(zyx, dhw)):\n",
    "        assert length % 2 == 0\n",
    "        # assert center < shape[i] # it's not necessary for \"moved center\"\n",
    "        low = round(center) - length // 2\n",
    "        high = round(center) + length // 2\n",
    "        if low < 0:\n",
    "            padding[i][0] = int(0 - low)\n",
    "            low = 0\n",
    "        if high > shape[i]:\n",
    "            padding[i][1] = int(high - shape[i])\n",
    "            high = shape[i]\n",
    "        crop_pos.append([int(low), int(high)])\n",
    "    cropped = voxel[crop_pos[0][0]:crop_pos[0][1], crop_pos[1]\n",
    "                                                   [0]:crop_pos[1][1], crop_pos[2][0]:crop_pos[2][1]]\n",
    "    if np.sum(padding) > 0:\n",
    "        cropped = np.lib.pad(cropped, padding, 'constant',\n",
    "                             constant_values=fill_with)\n",
    "    return cropped\n",
    "\n",
    "\n",
    "def window_clip(v, window_low=-1024, window_high=400, dtype=np.uint8):\n",
    "    '''Use lung windown to map CT voxel to grey.'''\n",
    "    # assert v.min() <= window_low\n",
    "    return np.round(np.clip((v - window_low) / (window_high - window_low) * 255., 0, 255)).astype(dtype)\n",
    "\n",
    "\n",
    "def resize(voxel, spacing, new_spacing=[1., 1., 1.]):\n",
    "    '''Resize `voxel` from `spacing` to `new_spacing`.'''\n",
    "    resize_factor = []\n",
    "    for sp, nsp in zip(spacing, new_spacing):\n",
    "        resize_factor.append(float(sp) / nsp)\n",
    "    resized = scipy.ndimage.interpolation.zoom(voxel, resize_factor, mode='nearest')\n",
    "    for i, (sp, shape, rshape) in enumerate(zip(spacing, voxel.shape, resized.shape)):\n",
    "        new_spacing[i] = float(sp) * shape / rshape\n",
    "    return resized, new_spacing\n",
    "\n",
    "\n",
    "def rotation(array, angle):\n",
    "    '''using Euler angles method.\n",
    "    @author: renchao\n",
    "    @params:\n",
    "        angle: 0: no rotation, 1: rotate 90 deg, 2: rotate 180 deg, 3: rotate 270 deg\n",
    "    '''\n",
    "    #\n",
    "    X = np.rot90(array, angle[0], axes=(0, 1))  # rotate in X-axis\n",
    "    Y = np.rot90(X, angle[1], axes=(0, 2))  # rotate in Y'-axis\n",
    "    Z = np.rot90(Y, angle[2], axes=(1, 2))  # rotate in Z\"-axis\n",
    "    return Z\n",
    "\n",
    "\n",
    "def reflection(array, axis):\n",
    "    '''\n",
    "    @author: renchao\n",
    "    @params:\n",
    "        axis: -1: no flip, 0: Z-axis, 1: Y-axis, 2: X-axis\n",
    "    '''\n",
    "    if axis != -1:\n",
    "        ref = np.flip(array, axis)\n",
    "    else:\n",
    "        ref = np.copy(array)\n",
    "    return ref\n",
    "\n",
    "\n",
    "def crop(array, zyx, dhw):\n",
    "    z, y, x = zyx\n",
    "    d, h, w = dhw\n",
    "    cropped = array[z - d // 2:z + d // 2,\n",
    "              y - h // 2:y + h // 2,\n",
    "              x - w // 2:x + w // 2]\n",
    "    return cropped\n",
    "\n",
    "\n",
    "def random_center(shape, move):\n",
    "    offset = np.random.randint(-move, move + 1, size=3)\n",
    "    # offset\n",
    "    zyx = np.array(shape) // 2 + offset\n",
    "    #np.array(shape) // 2对应的是中心点，+offset表示的是中心点的偏移量\n",
    "    #最后得出的是，中心的偏移量\n",
    "    return zyx\n",
    "\n",
    "\n",
    "def get_uniform_assign(length, subset):\n",
    "    assert subset > 0\n",
    "    per_length, remain = divmod(length, subset)\n",
    "    total_set = np.random.permutation(list(range(subset)) * per_length)\n",
    "    remain_set = np.random.permutation(list(range(subset)))[:remain]\n",
    "    return list(total_set) + list(remain_set)\n",
    "\n",
    "\n",
    "def split_validation(df, subset, by):\n",
    "    df = df.copy()\n",
    "    for sset in df[by].unique():\n",
    "        length = (df[by] == sset).sum()\n",
    "        df.loc[df[by] == sset, 'subset'] = get_uniform_assign(length, subset)\n",
    "    df['subset'] = df['subset'].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, collections.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "\n",
    "    return parse\n",
    "_single = _ntuple(1)\n",
    "_pair = _ntuple(2)\n",
    "_triple = _ntuple(3)\n",
    "_quadruple = _ntuple(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'activation': lambda: Activation('relu'),  # the activation functions\n",
    "    'bn_scale': True,  # whether to use the scale function in BN\n",
    "    'weight_decay': 0.,  # l2 weight decay\n",
    "    'kernel_initializer': 'he_uniform',  # initialization\n",
    "    'first_scale': lambda x: x / 128. - 1.,  # the first pre-processing function\n",
    "    'dhw': [32, 32, 32],  # the input shape\n",
    "    'k': 16,  # the `growth rate` in DenseNet\n",
    "    'bottleneck': 4,  # the `bottleneck` in DenseNet\n",
    "    'compression': 2,  # the `compression` in DenseNet\n",
    "    'first_layer': 32,  # the channel of the first layer\n",
    "    'down_structure': [2, 2, 2],  # the down-sample structure\n",
    "    'output_size': 1,  # the output number of the classification head\n",
    "    'dropout_rate': None  # whether to use dropout, and how much to use\n",
    "}\n",
    "\n",
    "\n",
    "def _conv_block(x, filters):\n",
    "    bn_scale = PARAMS['bn_scale']\n",
    "    activation = PARAMS['activation']\n",
    "    #激活函数\n",
    "    kernel_initializer = PARAMS['kernel_initializer']\n",
    "    #初始化内核\n",
    "    weight_decay = PARAMS['weight_decay']\n",
    "    #\n",
    "    bottleneck = PARAMS['bottleneck']\n",
    "    #bottleneck的结构\n",
    "    dropout_rate = PARAMS['dropout_rate']\n",
    "    #droupout 率\n",
    "    x = BatchNormalization(scale=bn_scale, axis=-1)(x)\n",
    "    #\n",
    "    x = activation()(x)\n",
    "    x = Conv3D(filters * bottleneck, kernel_size=(1, 1, 1), padding='same', use_bias=False,\n",
    "               kernel_initializer=kernel_initializer,\n",
    "               kernel_regularizer=l2_penalty(weight_decay))(x)\n",
    "    if dropout_rate is not None:\n",
    "        x = SpatialDropout3D(dropout_rate)(x)\n",
    "    x = BatchNormalization(scale=bn_scale, axis=-1)(x)\n",
    "    x = activation()(x)\n",
    "    x = Conv3D(filters, kernel_size=(3, 3, 3), padding='same', use_bias=True,\n",
    "               kernel_initializer=kernel_initializer,\n",
    "               kernel_regularizer=l2_penalty(weight_decay))(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _dense_block(x, n):\n",
    "    k = PARAMS['k']\n",
    "\n",
    "    for _ in range(n):\n",
    "        conv = _conv_block(x, k)\n",
    "        x = concatenate([conv, x], axis=-1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _transmit_block(x, is_last):\n",
    "    bn_scale = PARAMS['bn_scale']\n",
    "    activation = PARAMS['activation']\n",
    "    kernel_initializer = PARAMS['kernel_initializer']\n",
    "    weight_decay = PARAMS['weight_decay']\n",
    "    compression = PARAMS['compression']\n",
    "\n",
    "    x = BatchNormalization(scale=bn_scale, axis=-1)(x)\n",
    "    x = activation()(x)\n",
    "    if is_last:\n",
    "        x = GlobalAvgPool3D()(x)\n",
    "    else:\n",
    "        *_, f = x.get_shape().as_list()\n",
    "        x = Conv3D(f // compression, kernel_size=(1, 1, 1), padding='same', use_bias=True,\n",
    "                   kernel_initializer=kernel_initializer,\n",
    "                   kernel_regularizer=l2_penalty(weight_decay))(x)\n",
    "        x = AveragePooling3D((2, 2, 2), padding='valid')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_model(weights=None, verbose=True, **kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        assert k in PARAMS\n",
    "        PARAMS[k] = v\n",
    "    if verbose:\n",
    "        print(\"Model hyper-parameters:\", PARAMS)\n",
    "\n",
    "    dhw = PARAMS['dhw']\n",
    "    #input scale 32*32*32\n",
    "    first_scale = PARAMS['first_scale']\n",
    "    #first preprocessing function 归一化\n",
    "    first_layer = PARAMS['first_layer']\n",
    "    #第一个卷积层的通道数\n",
    "    kernel_initializer = PARAMS['kernel_initializer']\n",
    "    #初始化\n",
    "    weight_decay = PARAMS['weight_decay']\n",
    "    #\n",
    "    down_structure = PARAMS['down_structure']\n",
    "    #降采样结构\n",
    "    output_size = PARAMS['output_size']\n",
    "    #output number of classifi\n",
    "    shape = dhw + [1]\n",
    "\n",
    "    inputs = Input(shape=shape) #[32,32,32,1]\n",
    "\n",
    "    if first_scale is not None:\n",
    "        scaled = Lambda(first_scale)(inputs)\n",
    "        \n",
    "    else:\n",
    "        scaled = inputs\n",
    "    conv = Conv3D(first_layer, kernel_size=(3, 3, 3), padding='same', use_bias=True,\n",
    "                  kernel_initializer=kernel_initializer,\n",
    "                  kernel_regularizer=l2_penalty(weight_decay))(scaled)\n",
    "    \n",
    "    downsample_times = len(down_structure)\n",
    "    #降采样次数对应降采样结构的长度\n",
    "    top_down = []\n",
    "    for l, n in enumerate(down_structure):\n",
    "        db = _dense_block(conv, n)\n",
    "        #conv对应的是一个卷积网络，n对应的是降采样的的数字，4\n",
    "        top_down.append(db)\n",
    "        conv = _transmit_block(db, l == downsample_times - 1)\n",
    "\n",
    "\n",
    "    if output_size == 1:\n",
    "        last_activation = 'sigmoid'\n",
    "    else:\n",
    "        last_activation = 'softmax'\n",
    "\n",
    "    clf_head = Dense(output_size, activation=last_activation,\n",
    "                     kernel_regularizer=l2_penalty(weight_decay),\n",
    "                     kernel_initializer=kernel_initializer,\n",
    "                     name='clf')(conv)\n",
    "\n",
    "    model = Model(inputs, clf_head)\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_compiled(loss={\"clf\": 'binary_crossentropy'},\n",
    "                 optimizer='adam',\n",
    "                 metrics={'clf': ['accuracy', precision, recall]},\n",
    "                 loss_weights={\"clf\": 1.}, weights=None, **kwargs):\n",
    "    model = get_model(weights=weights, **kwargs)\n",
    "    model.compile(loss=loss, optimizer=optimizer,\n",
    "                  metrics=metrics, loss_weights=loss_weights)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########加载数据部分#########\n",
    "train_dataset=[]\n",
    "val_dataset=[]\n",
    "##第一个位置表示存的是0label的数据集，第二个位置，表示存的是1label的数据集\n",
    "#train_val=np.array(pd.read_csv(\"/kaggle/input/sjtu-ee228-2020/train_val.csv\"))\n",
    "train_val=np.array(pd.read_csv(\"train_val.csv\"))\n",
    "val=random.sample(range(0,465),33)\n",
    "#获取验证集的下标\n",
    "for dirname, _, filenames in os.walk('/M3DV'): #/kaggle/input\n",
    "    if dirname=='/M3DV/train_val/train_val':#'/kaggle/input/sjtu-ee228-2020/train_val/train_val':\n",
    "        count=0\n",
    "        for filename in filenames:\n",
    "            ID=filename[:-4]#已知name\n",
    "            result=[train_val[i][1] for i in range(len(filenames)) if train_val[i][0]==ID]\n",
    "            label=int(result[0])\n",
    "            if label==1:\n",
    "                label=random.uniform(0.65,0.7)\n",
    "            if label==0:\n",
    "                label=random.uniform(0.3,0.35)\n",
    "            data_path=os.path.join(dirname, filename)\n",
    "            tmp=np.load(data_path)\n",
    "            if count in val:\n",
    "                #print(count)\n",
    "                val_dataset.append((tmp['voxel'][33:65,33:65,33:65],label))\n",
    "                #valS.append(tmp['seg'][33:65,33:65,33:65])\n",
    "            train_dataset.append((tmp['voxel'][33:65,33:65,33:65],label))\n",
    "            count+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "a=random.uniform(0.1,0.5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  mix操作########(实际实验没有使用)\n",
    "train_dataset_new=[]\n",
    "alpha=0.5\n",
    "for i in range(1600):\n",
    "    number=random.sample(range(0,465),2)\n",
    "    lam=np.random.random()\n",
    "    x1=train_dataset[number[0]][0]\n",
    "    x2=train_dataset[number[1]][0]\n",
    "    y1=train_dataset[number[0]][1]\n",
    "    y2=train_dataset[number[1]][1]\n",
    "    x=lam*x1+(1-lam)*x2\n",
    "    y=lam*y1+(1-lam)*y2\n",
    "    train_dataset_new.append((x,y))\n",
    "train_dataset=train_dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####关于数据loader的函数#######\n",
    "def shuffle_iterator(iterator):\n",
    "    # iterator should have limited size\n",
    "    #interator是一个范围的数\n",
    "    index = list(iterator)\n",
    "    total_size = len(index)\n",
    "    i = 0\n",
    "    random.shuffle(index)\n",
    "    #随机打乱下标\n",
    "    while True:\n",
    "        yield index[i]\n",
    "        i += 1\n",
    "        if i >= total_size:\n",
    "            i = 0\n",
    "            random.shuffle(index)\n",
    "def _collate_fn(data):\n",
    "        xs = []\n",
    "        ys = []\n",
    "        #segs=[]\n",
    "        for x, y in data:\n",
    "            #print(\"y:\",y)\n",
    "            xs.append(x)\n",
    "            ys.append(y)#对应的是label，\n",
    "        xs=np.array(xs)\n",
    "        ys=np.array(ys)\n",
    "        xs = xs.reshape(xs.shape[0],32,32,32,1)\n",
    "        #ys = np_utils.to_categorical(ys,num_classes=2)\n",
    "        return xs,ys\n",
    "def get_loader(dataset, batch_size):\n",
    "    total_size = len(dataset)\n",
    "    #total_size表示整个数据集的大小\n",
    "    print('Size', total_size)\n",
    "    index_generator = shuffle_iterator(range(total_size))\n",
    "    while True:\n",
    "        data = []\n",
    "        for _ in range(batch_size):\n",
    "            idx = next(index_generator)\n",
    "            data.append(dataset[idx])\n",
    "        #print(np.array(data).shape)\n",
    "        #data 对应的是一个batch_size数据合集\n",
    "        yield _collate_fn(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes=16\n",
    "batch_size=8\n",
    "crop_size=[32, 32,32]\n",
    "random_move=3\n",
    "learning_rate=1.e-4\n",
    "segmentation_task_ratio=0.2\n",
    "weight_decay=0.\n",
    "save_folder='test'\n",
    "epochs=16\n",
    "\n",
    "train_loader = get_loader(train_dataset, batch_size=batch_sizes)\n",
    "    \n",
    "val_loader = get_loader(val_dataset, batch_size=batch_size)\n",
    "model = get_compiled(output_size=1)\n",
    "checkpointer = ModelCheckpoint(filepath='tmp/%s/weights.{epoch:02d}.h5' % save_folder, verbose=1,\n",
    "                                   period=1, save_weights_only=True)\n",
    "best_keeper = ModelCheckpoint(filepath='tmp/%s/best.h5' % save_folder, verbose=1, save_weights_only=True,\n",
    "                                  monitor='val_clf_acc', save_best_only=True, period=1, mode='max')\n",
    "                                  \n",
    "csv_logger = CSVLogger('tmp/%s/training.csv' % save_folder)\n",
    "    \n",
    "tensorboard = TensorBoard(log_dir='tmp/%s/logs/' % save_folder)\n",
    "model.fit_generator(generator=train_loader, steps_per_epoch=len(train_dataset), max_queue_size=500, workers=1,\n",
    "                        validation_data=val_loader, epochs=epochs, validation_steps=len(val_dataset),callbacks=[checkpointer,best_keeper,csv_logger, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######加载训练好的数据集#######\n",
    "\n",
    "batch_sizes=16\n",
    "batch_size=8\n",
    "crop_size=[32, 32,32]\n",
    "random_move=3\n",
    "learning_rate=1.e-4\n",
    "segmentation_task_ratio=0.2\n",
    "weight_decay=0.\n",
    "save_folder='test'\n",
    "epochs=16\n",
    "\n",
    "#train_loader = get_loader(train_dataset, batch_size=batch_sizes)\n",
    "    \n",
    "#val_loader = get_loader(val_dataset, batch_size=batch_size)\n",
    "model = get_compiled(output_size=1)\n",
    "model.load_weights(r\"weights.15.h5\")#/kaggle/input/model-trained/weights.15.h5\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[]\n",
    "for dirname, _, filenames in os.walk('/M3DV'):#/kaggle/input\n",
    "    if dirname=='/M3DV/test':#/kaggle/input/sjtu-ee228-2020/test/test\n",
    "        #print(len(filenames))\n",
    "        count=0\n",
    "        for filename in filenames:\n",
    "            ID=filename[9:-4]#已知name\n",
    "            data_path=os.path.join(dirname, filename)\n",
    "            tmp=np.load(data_path)\n",
    "            xs=np.array(tmp['voxel'][33:65,33:65,33:65])\n",
    "            #print(xs.shape)\n",
    "            xs=xs.reshape(1,32,32,32,1)\n",
    "            test.append((int(ID),xs))    \n",
    "test=sorted(test,key=lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1=[]\n",
    "result2=[]\n",
    "result3=[]\n",
    "for term in test:\n",
    "    ID=term[0]\n",
    "    name=\"candidate\"+str(ID)\n",
    "    result=model.predict(term[1],batch_size=1)\n",
    "    result1.append((name,result[0][0]))\n",
    "    #result2.append((name,(result[0][0]+result[0][1])/2))\n",
    "    #result3.append((name,result[0][1]))\n",
    "import csv\n",
    "with open(\"predict.csv\",'w') as f:\n",
    "    f_csv=csv.writer(f)\n",
    "    f_csv.writerow([\"name\",\"predicted\"])\n",
    "    f_csv.writerows(result1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
